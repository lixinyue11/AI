{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-16T06:06:08.021456Z",
     "start_time": "2025-07-16T06:06:08.003216Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "api_key = \"sk-07b0bbfd24bf4cb391cad5da8da05f6f\"\n",
    "base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "model_name = \"text-embedding-v3\"\n",
    "BATCH_SIZE  = 32\n",
    "TOP_K       = 3  \n",
    "\n",
    "# from langchain_community.embeddings import DashScopeEmbeddings\n",
    "# embeddings = DashScopeEmbeddings(\n",
    "#     model=\"text-embedding-v2\",\n",
    "#     dashscope_api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\")\n",
    "# llm = ChatOpenAI(base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "#                  api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\",\n",
    "#                  model=\"qwen2.5-72b-instruct\", temperature=0.7)\n",
    "# client_openai = OpenAI(\n",
    "#     api_key= \"sk-07b0bbfd24bf4cb391cad5da8da05f6f\",\n",
    "#     base_url= \"https://dashscope.aliyuncs.com/compatible-mode/v1\" \n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dualâ€‘Encoder æ£€ç´¢",
   "id": "7add97fdc90a653c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:08:04.230238Z",
     "start_time": "2025-07-16T06:08:03.424007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "        å’Œä»¥ä¸‹çš„åŒºåˆ«ï¼Œè¿™é‡Œå¯ä»¥åšæˆLLMç”Ÿæˆç­”æ¡ˆï¼Œæ”¯æŒæ”¯æŒ IndexIVFFlat å¸¦è®­ç»ƒçš„å€’æ’ç´¢å¼•\n",
    "        quantizer = faiss.IndexFlatIP(dim)\n",
    "        index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "        if not index.is_trained:index.train(vectors)\n",
    "        index.add(vectors)\n",
    "'''\n",
    "''' åªåšæ£€ç´¢,ä½¿ç”¨ IndexFlatIP æˆ–å½’ä¸€åŒ–ååšä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢,å¯æ‰©å±•ä¸ºå¤šæ–‡æ¡£ç»“æ„\n",
    " faiss.normalize_L2(doc_vectors)\n",
    "dim = doc_vectors.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)           # å†…ç§¯ï¼Œå½’ä¸€åŒ–åâ‰ˆä½™å¼¦\n",
    "index.add(doc_vectors)\n",
    " '''\n",
    "api_key = \"sk-07b0bbfd24bf4cb391cad5da8da05f6f\"\n",
    "base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "model_name = \"text-embedding-v3\"\n",
    "BATCH_SIZE  = 32\n",
    "TOP_K       = 3\n",
    "embedding_client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=base_url\n",
    "    )\n",
    "def embed_texts(texts):\n",
    "    resp = embedding_client.embeddings.create(\n",
    "            model=model_name,\n",
    "            input=batch,\n",
    "            #dimensions=dimensions,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "    # OpenAI v1 è¿”å›é¡ºåºä¸è¾“å…¥ä¿æŒä¸€è‡´\n",
    "    return np.array([d.embedding for d in resp.data], dtype=\"float32\")\n",
    "\n",
    "documents = [\n",
    "    \"RAG æ˜¯ Retrievalâ€‘Augmented Generation çš„ç¼©å†™ã€‚\",\n",
    "    \"Dualâ€‘Encoder é‡‡ç”¨æŸ¥è¯¢å’Œæ–‡æ¡£ä¸¤ä¸ªå¡”ç‹¬ç«‹ç¼–ç ã€‚\",\n",
    "    \"OpenAI æä¾› textâ€‘embeddingâ€‘3 ç³»åˆ—æ¨¡å‹ã€‚\",\n",
    "    \"åœ¨å›¾è°±æ£€ç´¢ä¸­å¸¸ç”¨å›¾æ•°æ®åº“ Neo4jã€‚\",\n",
    "]\n",
    "doc_vectors = []\n",
    "for i in tqdm(range(0, len(documents), BATCH_SIZE)):\n",
    "    batch = documents[i:i+BATCH_SIZE]\n",
    "    doc_vectors.append(embed_texts(batch))\n",
    "\n",
    "doc_vectors = np.vstack(doc_vectors)\n",
    "\n",
    "# â”€â”€ 2.3 å¯é€‰ï¼šä½™å¼¦æ£€ç´¢ â†’ å…ˆåš L2 å½’ä¸€åŒ–\n",
    "faiss.normalize_L2(doc_vectors)\n",
    "\n",
    "# â”€â”€ 2.4 å»ºç«‹ FAISS ç´¢å¼•ï¼ˆä½™å¼¦ = å½’ä¸€åŒ–å L2ï¼‰\n",
    "dim = doc_vectors.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)           # å†…ç§¯ï¼Œå½’ä¸€åŒ–åâ‰ˆä½™å¼¦\n",
    "index.add(doc_vectors)\n",
    "def search(query, k=TOP_K):\n",
    "    # 3.1 ç¼–ç æŸ¥è¯¢\n",
    "    q_vec = embed_texts([query])\n",
    "    faiss.normalize_L2(q_vec)\n",
    "\n",
    "    # 3.2 ANN æ£€ç´¢\n",
    "    scores, indices = index.search(q_vec, k)\n",
    "    return [(documents[i], float(scores[0][j])) for j, i in enumerate(indices[0])]\n",
    "\n",
    "# æµ‹è¯•\n",
    "query = \"å¦‚ä½•ç”¨åŒå¡”åšè¯­ä¹‰æ£€ç´¢ï¼Ÿ\"\n",
    "for rank, (text, score) in enumerate(search(query), 1):\n",
    "    print(f\"{rank}. {text}  (similarity={score:.4f})\")"
   ],
   "id": "6c91d39b081e5ad6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. RAG æ˜¯ Retrievalâ€‘Augmented Generation çš„ç¼©å†™ã€‚  (similarity=1.0000)\n",
      "2. Dualâ€‘Encoder é‡‡ç”¨æŸ¥è¯¢å’Œæ–‡æ¡£ä¸¤ä¸ªå¡”ç‹¬ç«‹ç¼–ç ã€‚  (similarity=0.4616)\n",
      "3. åœ¨å›¾è°±æ£€ç´¢ä¸­å¸¸ç”¨å›¾æ•°æ®åº“ Neo4jã€‚  (similarity=0.4595)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multiâ€‘vector æ£€ç´¢\n",
    "### ä¸ºæ¯ä¸€ä¸ªæ–‡æ¡£å—åˆ›å»ºå­åˆ†å—ï¼Œ åŸºäºå­åˆ†å—å’ŒåŸå§‹æ–‡æ¡£å—è¿›è¡ŒåŒ¹é…ï¼Œ è¿”å›åŸå§‹æ–‡æ¡£å—ã€‚\n",
    "### ä¸ºæ¯ä¸€ä¸ªæ–‡æ¡£å—åˆ›å»ºæ‘˜è¦(Summary)ï¼Œ åŸºäºæ‘˜è¦å’ŒåŸå§‹æ–‡æ¡£å—è¿›è¡ŒåŒ¹é…ï¼Œ è¿”å›åŸå§‹æ–‡æ¡£å—ã€‚\n",
    "### ä¸ºæ¯ä¸€ä¸ªæ–‡æ¡£å—åˆ›å»ºå‡è®¾æŸ¥è¯¢(Hypothetical Questions)ï¼Œ åŸºäºå‡è®¾æŸ¥è¯¢å’ŒåŸå§‹æ–‡æ¡£å—è¿›è¡ŒåŒ¹é…ï¼Œ è¿”å›åŸå§‹æ–‡æ¡£å—ã€‚"
   ],
   "id": "e8ff81378177309b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:31:42.245706Z",
     "start_time": "2025-07-16T06:31:42.238505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# å¼•å…¥uuidåº“ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦\n",
    "import uuid\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "loaders = [TextLoader('æ›²é¢æ‰“å°æœºè¯´æ˜ä¹¦.txt', encoding='utf-8') ,TextLoader('text.txt', encoding='utf-8') ]\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\",\n",
    "    dashscope_api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\")\n",
    "llm = ChatOpenAI(base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "                 api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\",\n",
    "                 model=\"qwen2.5-72b-instruct\", temperature=0.7)\n",
    "# åˆ›å»ºä¸€ä¸ªRecursiveCharacterTextSplitterå¯¹è±¡ï¼Œç”¨äºåˆ†å‰²æ–‡æœ¬\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000)\n",
    "# ä½¿ç”¨TextLoaderå¯¹è±¡åŠ è½½æ–‡æœ¬æ–‡ä»¶\n",
    "# docs = loader.load()\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "# ä½¿ç”¨RecursiveCharacterTextSplitterå¯¹è±¡åˆ†å‰²æ–‡æ¡£\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸º\"doc\"ï¼Œå€¼ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°è¿”å›æ–‡æ¡£çš„å†…å®¹\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # ä½¿ç”¨ChatPromptTemplateç±»çš„from_templateæ–¹æ³•åˆ›å»ºä¸€ä¸ªèŠå¤©æç¤ºæ¨¡æ¿\n",
    "    | ChatPromptTemplate.from_template(\"æ€»ç»“ä»¥ä¸‹æ–‡æ¡£:\\n\\n{doc}\")\n",
    "    # åˆ›å»ºä¸€ä¸ªChatOpenAIå¯¹è±¡ï¼Œæœ€å¤§é‡è¯•æ¬¡æ•°ä¸º0\n",
    "    |llm\n",
    "    # åˆ›å»ºä¸€ä¸ªStrOutputParserå¯¹è±¡\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# ä½¿ç”¨chainå¯¹è±¡çš„batchæ–¹æ³•æ‰¹é‡å¤„ç†æ–‡æ¡£ï¼Œæœ€å¤§å¹¶å‘æ•°ä¸º5\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªChromaå¯¹è±¡ï¼Œé›†åˆåä¸º\"summaries\"ï¼ŒåµŒå…¥å‡½æ•°ä¸ºOpenAIEmbeddings()\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=embeddings)\n",
    "# åˆ›å»ºä¸€ä¸ªInMemoryByteStoreå¯¹è±¡ï¼Œç”¨äºå­˜å‚¨å­—èŠ‚æ•°æ®\n",
    "store = InMemoryByteStore()\n",
    "# å®šä¹‰ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå€¼ä¸º\"doc_id\"\n",
    "id_key = \"doc_id\"\n",
    "# åˆ›å»ºä¸€ä¸ªMultiVectorRetrieverå¯¹è±¡\n",
    "retriever = MultiVectorRetriever(\n",
    "    # è®¾ç½®å‘é‡å­˜å‚¨ä¸ºvectorstoreå¯¹è±¡\n",
    "    vectorstore=vectorstore,\n",
    "    # è®¾ç½®å­—èŠ‚å­˜å‚¨ä¸ºstoreå¯¹è±¡\n",
    "    byte_store=store,\n",
    "    # è®¾ç½®idé”®ä¸º\"id_key\"\n",
    "    id_key=id_key,\n",
    ")\n",
    "# ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ID\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "# åˆ›å»ºä¸€ä¸ªDocumentå¯¹è±¡ï¼Œé¡µé¢å†…å®¹ä¸ºsï¼Œå…ƒæ•°æ®ä¸º{id_key: doc_ids[i]}\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    # å¯¹summariesä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ å’Œå®ƒçš„ç´¢å¼•è¿›è¡Œè¿­ä»£\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "# ä½¿ç”¨retrieverå¯¹è±¡çš„vectorstoreå±æ€§çš„add_documentsæ–¹æ³•æ·»åŠ æ–‡æ¡£\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "# ä½¿ç”¨retrieverå¯¹è±¡çš„docstoreå±æ€§çš„msetæ–¹æ³•è®¾ç½®æ–‡æ¡£\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "# ä½¿ç”¨vectorstoreå¯¹è±¡çš„similarity_searchæ–¹æ³•æœç´¢ä¸\"å¦‚ä½•å†™æ–‡ç« \"ç›¸ä¼¼çš„æ–‡æ¡£\n",
    "sub_docs = vectorstore.similarity_search(\"å¼ ä¸‰\")\n",
    "# è·å–æœç´¢ç»“æœçš„ç¬¬ä¸€ä¸ªæ–‡æ¡£\n",
    "print(sub_docs[0] )\n",
    "# ä½¿ç”¨retrieverå¯¹è±¡çš„get_relevant_documentsæ–¹æ³•è·å–ä¸\"å¦‚ä½•å†™æ–‡ç« \"ç›¸å…³çš„æ–‡æ¡£\n",
    "retrieved_docs = retriever.get_relevant_documents(\"å¼ ä¸‰\")\n",
    "# è·å–æœç´¢ç»“æœçš„ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å†…å®¹\n",
    "print(retrieved_docs[0].page_content  )\n"
   ],
   "id": "6d0c5dab1db61ed6",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RGA QUERYæ”¹å†™\n",
    "### é€šè¿‡å¯¹åŸå§‹æŸ¥è¯¢è¿›è¡Œè¯­ä¹‰æ‰©å±•æˆ–æ”¹å†™åï¼Œæå‡å¬å›æ•ˆæœï¼Œé€‚ç”¨äº Dense æ£€ç´¢ã€å¤šå‘é‡æ£€ç´¢ã€RAG ç­‰åœºæ™¯"
   ],
   "id": "b42bdfe2b6c5faae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:31:54.628014Z",
     "start_time": "2025-07-16T06:31:54.613499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "# ===================== é…ç½®åŒºåŸŸ =====================\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\",\n",
    "    dashscope_api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\")\n",
    "llm = ChatOpenAI(base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "                 api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\",\n",
    "                 model=\"qwen2.5-72b-instruct\", temperature=0.7)\n",
    "client_openai = OpenAI(\n",
    "    api_key= \"sk-07b0bbfd24bf4cb391cad5da8da05f6f\",\n",
    "    base_url= \"https://dashscope.aliyuncs.com/compatible-mode/v1\" \n",
    ")\n",
    "EXPANSION_NUM = 3   # RGAç”Ÿæˆå‡ ä¸ªæ”¹å†™æŸ¥è¯¢\n",
    "TOP_K = 4           # æ¯ä¸ªå­æŸ¥è¯¢å¬å›Kä¸ªæ–‡æ¡£\n",
    "\n",
    "# ===================== åŠ è½½æ–‡æ¡£å¹¶æ„å»ºç´¢å¼• =====================\n",
    "def build_faiss_index(filepath):\n",
    "    loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "   \n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# ===================== æ„å»ºRGAæŸ¥è¯¢æ‰©å……é“¾ =====================\n",
    "def build_query_expansion_chain():\n",
    "   # llm = ChatOpenAI(model=client_openai, temperature=0.7)\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢å¢å¼ºåŠ©æ‰‹ï¼Œè¯·åŸºäºç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢ï¼Œç”Ÿæˆ {num} ä¸ªä¸åŒè§’åº¦ä½†ç›¸å…³çš„æ£€ç´¢æŸ¥è¯¢ï¼Œå¢å¼ºè¯­ä¹‰å¤šæ ·æ€§ã€‚\n",
    "\n",
    "åŸå§‹æŸ¥è¯¢ï¼š{query}\n",
    "\n",
    "è¯·è¾“å‡ºä¸€ç»„æ”¹å†™æŸ¥è¯¢ï¼š\n",
    "\"\"\")\n",
    "    return LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# ===================== å¤šæŸ¥è¯¢æ£€ç´¢å¹¶åˆå¹¶ç»“æœ =====================\n",
    "def rga_retrieve(query, vectorstore, expander, top_k=4):\n",
    "    # 1. RGAç”Ÿæˆæ”¹å†™æŸ¥è¯¢\n",
    "    expansion_prompt = expander.run({\"query\": query, \"num\": EXPANSION_NUM})\n",
    "    reformulated_queries = [line.strip(\" 123456.-\").strip() \n",
    "                            for line in expansion_prompt.strip().split(\"\\n\") if line.strip()]\n",
    "    \n",
    "    print(f\"\\nğŸ§  åŸå§‹æŸ¥è¯¢: {query}\")\n",
    "    print(\"ğŸ” æ”¹å†™æŸ¥è¯¢:\")\n",
    "    for i, rq in enumerate(reformulated_queries):\n",
    "        print(f\"  [{i+1}] {rq}\")\n",
    "\n",
    "    # 2. æ¯ä¸ªå­æŸ¥è¯¢åˆ†åˆ«æ£€ç´¢\n",
    "    all_results = []\n",
    "    for rq in reformulated_queries:\n",
    "        results = vectorstore.similarity_search(rq, k=top_k)\n",
    "        all_results.extend(results)\n",
    "\n",
    "    # 3. å»é‡ï¼ˆå¯é€‰ï¼šå†æ’åºï¼‰\n",
    "    seen = set()\n",
    "    unique_results = []\n",
    "    for d in all_results:\n",
    "        if d.page_content not in seen:\n",
    "            seen.add(d.page_content)\n",
    "            unique_results.append(d)\n",
    "\n",
    "    return unique_results[:top_k * EXPANSION_NUM]\n",
    "\n",
    "# ===================== ä¸»æµç¨‹ =====================\n",
    "if __name__ == \"__main__\":\n",
    "    vs = build_faiss_index(\"æ›²é¢æ‰“å°æœºè¯´æ˜ä¹¦.txt\")\n",
    "    expander = build_query_expansion_chain()\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nè¯·è¾“å…¥æŸ¥è¯¢ï¼ˆç©ºé€€å‡ºï¼‰ï¼š\").strip()\n",
    "        if not query:\n",
    "            break\n",
    "        results = rga_retrieve(query, vs, expander)\n",
    "\n",
    "        print(f\"\\nğŸ“š å…±è¿”å› {len(results)} æ¡ç»“æœï¼š\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"\\n[{i+1}] {doc.metadata.get('source', '')}\")\n",
    "            print(doc.page_content[:200] + \"â€¦\")\n"
   ],
   "id": "a02fa3c386434767",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RGA å±‚çº§ç¨ å¯†æ£€ç´¢ä»£ç ",
   "id": "9585e52fb53fa9a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:42:08.813027Z",
     "start_time": "2025-07-16T09:42:03.944395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, faiss, numpy as np\n",
    "from typing import Dict\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ====== é…ç½® ======\n",
    "DASH_API_KEY = \"sk-0b8d48b85f1742829ef3032133375d3e\"\n",
    "EMBED_MODEL = \"text-embedding-v2\"\n",
    "LLM_MODEL = \"qwen2.5-72b-instruct\"\n",
    "EMBED_DIM = 256\n",
    "DOC_EMB   = DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=DASH_API_KEY)\n",
    "CHUNK_EMB = DOC_EMB\n",
    "LLM       =  ChatOpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "                 api_key=DASH_API_KEY,\n",
    "                 model=LLM_MODEL, temperature=0.7)\n",
    "L1, L2 = 3, 4                     # doc / chunk topâ€‘k\n",
    "EXP_N   = 3                       # RGA æ”¹å†™æ•°\n",
    "\n",
    "\n",
    "SPLITTER= RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=20)\n",
    "\n",
    "# ====== 1. è‡ªå¸¦å¾®å‹è¯­æ–™ ======\n",
    "RAW = [\n",
    "    \"æ‰“å°æœºåº”åœ¨é€šé£ç¯å¢ƒä¸‹ä½¿ç”¨ï¼Œé˜²æ­¢è®¾å¤‡è¿‡çƒ­å¹¶é™ä½ç«ç¾é£é™©ã€‚\",\n",
    "    \"ç»´ä¿®æ‰“å°æœºå‰å¿…é¡»æ–­ç”µï¼Œä»¥é¿å…è§¦ç”µå’Œæœºæ¢°ä¼¤å®³ã€‚\",\n",
    "    \"èŠ‚èƒ½æ¨¡å¼å¯è®©æ‰“å°æœºåŠŸè€—é™ä½ 30%ã€‚\"\n",
    "]\n",
    "docs   = [Document(page_content=t, metadata={\"id\": f\"d{i}\"}) for i, t in enumerate(RAW)]\n",
    "chunks = SPLITTER.split_documents(docs)\n",
    "\n",
    "# ====== 2. å»ºåŒç´¢å¼• ======\n",
    "doc_vec = np.array(DOC_EMB.embed_documents([d.page_content for d in docs]), dtype=\"float32\")\n",
    "chk_vec = np.array(CHUNK_EMB.embed_documents([c.page_content for c in chunks]), dtype=\"float32\")\n",
    "for v in (doc_vec, chk_vec): faiss.normalize_L2(v)\n",
    "dim = doc_vec.shape[1]          # â† è‡ªåŠ¨æŠ“å–çœŸå®ç»´åº¦ï¼ˆåº”ä¸º 1536ï¼‰\n",
    "doc_idx = faiss.IndexFlatIP(dim)\n",
    "doc_idx.add(doc_vec)\n",
    "\n",
    "chk_idx = faiss.IndexFlatIP(dim)\n",
    "chk_idx.add(chk_vec)\n",
    "\n",
    "chk2doc = [c.metadata[\"id\"] for c in chunks]\n",
    "id2doc  = {d.metadata[\"id\"]: d for d in docs}\n",
    "\n",
    "# ====== 3. RGA æ‰©å†™å™¨ ======\n",
    "expander = LLMChain(\n",
    "    llm=LLM,\n",
    "    prompt=PromptTemplate.from_template(\n",
    "        \"ç»™ç”¨æˆ·æŸ¥è¯¢æ‰©å†™ {n} ä¸ªä¸åŒè¡¨è¾¾ï¼Œæ¯è¡Œä¸€æ¡ï¼š\\næŸ¥è¯¢ï¼š{q}\\næ‰©å†™ï¼š\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ====== 4. å±‚çº§æ£€ç´¢å‡½æ•° ======\n",
    "def search(q: str):\n",
    "    exps = expander.run({\"q\": q, \"n\": EXP_N})\n",
    "    queries = [q] + [l.strip(\" 123456.-\") for l in exps.split(\"\\n\") if l.strip()]\n",
    "    print(\"ğŸ” æ‰©å±•æŸ¥è¯¢:\", queries)\n",
    "\n",
    "    # æ–‡æ¡£çº§\n",
    "    qv = np.array([DOC_EMB.embed_query(x) for x in queries], dtype=\"float32\"); faiss.normalize_L2(qv)\n",
    "    _, I = doc_idx.search(qv, L1)\n",
    "    cand_ids = {docs[i].metadata[\"id\"] for i in I.flatten()}\n",
    "\n",
    "    # chunk çº§\n",
    "    qv2 = np.array([CHUNK_EMB.embed_query(x) for x in queries], dtype=\"float32\"); faiss.normalize_L2(qv2)\n",
    "    D, I = chk_idx.search(qv2, L2)\n",
    "\n",
    "    scores: Dict[str, float] = {}\n",
    "    for ds, is_ in zip(D, I):\n",
    "        for s, idx in zip(ds, is_):\n",
    "            did = chk2doc[idx]\n",
    "            if did in cand_ids:\n",
    "                scores[did] = max(scores.get(did, -1), s)\n",
    "\n",
    "    return sorted([(id2doc[k], v) for k, v in scores.items()], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ====== 5. Demo è¿è¡Œ ======\n",
    "for doc, sc in search(\"æ‰“å°æœºå®‰å…¨éšæ‚£\")[:3]:\n",
    "    print(f\"\\nScore={sc:.3f} | {doc.page_content}\")\n"
   ],
   "id": "13a98855839ac3f1",
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mConnectError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:72\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[1;34m()\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 72\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:236\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[1;32m--> 236\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[1;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[1;32m--> 236\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[0;32m    242\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpcore\\_sync\\http_proxy.py:288\u001B[0m, in \u001B[0;36mTunnelHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    282\u001B[0m connect_request \u001B[38;5;241m=\u001B[39m Request(\n\u001B[0;32m    283\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCONNECT\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    284\u001B[0m     url\u001B[38;5;241m=\u001B[39mconnect_url,\n\u001B[0;32m    285\u001B[0m     headers\u001B[38;5;241m=\u001B[39mconnect_headers,\n\u001B[0;32m    286\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    287\u001B[0m )\n\u001B[1;32m--> 288\u001B[0m connect_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconnect_request\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connect_response\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m200\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m connect_response\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m299\u001B[39m:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection\u001B[38;5;241m.\u001B[39mhandle_request(request)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m     ssl_object \u001B[38;5;241m=\u001B[39m stream\u001B[38;5;241m.\u001B[39mget_extra_info(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mssl_object\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:156\u001B[0m, in \u001B[0;36mHTTPConnection._connect\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart_tls\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m--> 156\u001B[0m     stream \u001B[38;5;241m=\u001B[39m stream\u001B[38;5;241m.\u001B[39mstart_tls(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    157\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m stream\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpcore\\_backends\\sync.py:154\u001B[0m, in \u001B[0;36mSyncStream.start_tls\u001B[1;34m(self, ssl_context, server_hostname, timeout)\u001B[0m\n\u001B[0;32m    150\u001B[0m exc_map: ExceptionMapping \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    151\u001B[0m     socket\u001B[38;5;241m.\u001B[39mtimeout: ConnectTimeout,\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;167;01mOSError\u001B[39;00m: ConnectError,\n\u001B[0;32m    153\u001B[0m }\n\u001B[1;32m--> 154\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\python310\\lib\\contextlib.py:153\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 153\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpcore\\_exceptions.py:14\u001B[0m, in \u001B[0;36mmap_exceptions\u001B[1;34m(map)\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[1;32m---> 14\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mConnectError\u001B[0m: EOF occurred in violation of protocol (_ssl.c:997)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mConnectError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\openai\\_base_client.py:972\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 972\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39msend(\n\u001B[0;32m    973\u001B[0m         request,\n\u001B[0;32m    974\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_stream_response_body(request\u001B[38;5;241m=\u001B[39mrequest),\n\u001B[0;32m    975\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    976\u001B[0m     )\n\u001B[0;32m    977\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpx\\_client.py:926\u001B[0m, in \u001B[0;36mClient.send\u001B[1;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[0;32m    924\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[1;32m--> 926\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    931\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpx\\_client.py:954\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[1;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[0;32m    953\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 954\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    955\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    957\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    959\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpx\\_client.py:991\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[1;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[0;32m    989\u001B[0m     hook(request)\n\u001B[1;32m--> 991\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    992\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpx\\_client.py:1027\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[1;32m-> 1027\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:235\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    223\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[0;32m    224\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    225\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    233\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    234\u001B[0m )\n\u001B[1;32m--> 235\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m    236\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool\u001B[38;5;241m.\u001B[39mhandle_request(req)\n",
      "File \u001B[1;32mD:\\python310\\lib\\contextlib.py:153\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 153\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:89\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[1;34m()\u001B[0m\n\u001B[0;32m     88\u001B[0m message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[1;32m---> 89\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[1;31mConnectError\u001B[0m: EOF occurred in violation of protocol (_ssl.c:997)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mAPIConnectionError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 121\u001B[0m\n\u001B[0;32m    118\u001B[0m expander \u001B[38;5;241m=\u001B[39m build_query_expander()\n\u001B[0;32m    119\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mæ‰“å°æœºçš„å®‰å…¨æ€§è®¾è®¡\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 121\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mhierarchical_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdoc_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdoc_path_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk2doc_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpander\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m5\u001B[39m]\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mğŸ¯ Top ç»“æœï¼š\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (doc, score) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(results, \u001B[38;5;241m1\u001B[39m):\n",
      "Cell \u001B[1;32mIn[25], line 82\u001B[0m, in \u001B[0;36mhierarchical_search\u001B[1;34m(query, doc_index, chunk_index, doc_path_map, chunk2doc_path, chunks, query_expander)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mhierarchical_search\u001B[39m(query: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m     75\u001B[0m                         doc_index,\n\u001B[0;32m     76\u001B[0m                         chunk_index,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     80\u001B[0m                         query_expander):\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;66;03m# (1) RGA æŸ¥è¯¢æ‰©å±•\u001B[39;00m\n\u001B[1;32m---> 82\u001B[0m     expansion \u001B[38;5;241m=\u001B[39m \u001B[43mquery_expander\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquery\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mEXPANSION_N\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     83\u001B[0m     queries \u001B[38;5;241m=\u001B[39m [query] \u001B[38;5;241m+\u001B[39m [q\u001B[38;5;241m.\u001B[39mstrip(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m 123456.-\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m expansion\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m q\u001B[38;5;241m.\u001B[39mstrip()]\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mğŸ” æ‰©å±•æŸ¥è¯¢ï¼š\u001B[39m\u001B[38;5;124m\"\u001B[39m, queries)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    187\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    188\u001B[0m     emit_warning()\n\u001B[1;32m--> 189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:603\u001B[0m, in \u001B[0;36mChain.run\u001B[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[0;32m    601\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    602\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 603\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[0;32m    604\u001B[0m         _output_key\n\u001B[0;32m    605\u001B[0m     ]\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    609\u001B[0m         _output_key\n\u001B[0;32m    610\u001B[0m     ]\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    187\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    188\u001B[0m     emit_warning()\n\u001B[1;32m--> 189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:386\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \n\u001B[0;32m    356\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    379\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[0;32m    381\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[0;32m    384\u001B[0m }\n\u001B[1;32m--> 386\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:167\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    166\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    168\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:157\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[0;32m    156\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 157\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    160\u001B[0m     )\n\u001B[0;32m    162\u001B[0m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    163\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[0;32m    164\u001B[0m     )\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py:127\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_call\u001B[39m(\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    124\u001B[0m     inputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    125\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    126\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m--> 127\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py:139\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[1;34m(self, input_list, run_manager)\u001B[0m\n\u001B[0;32m    137\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[1;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    140\u001B[0m         prompts,\n\u001B[0;32m    141\u001B[0m         stop,\n\u001B[0;32m    142\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    143\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs,\n\u001B[0;32m    144\u001B[0m     )\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[0;32m    147\u001B[0m         cast(\u001B[38;5;28mlist\u001B[39m, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[0;32m    148\u001B[0m     )\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    949\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    950\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    954\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    955\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    956\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 957\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[0;32m    774\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    775\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 776\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    777\u001B[0m                 m,\n\u001B[0;32m    778\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    779\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    780\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    781\u001B[0m             )\n\u001B[0;32m    782\u001B[0m         )\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1020\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1022\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m   1023\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m   1024\u001B[0m     )\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1026\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1071\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1069\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response\u001B[38;5;241m.\u001B[39mheaders)}\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1071\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n\u001B[0;32m   1072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response, generation_info)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\openai\\_utils\\_utils.py:287\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    285\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    882\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    883\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    884\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    922\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    923\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m    924\u001B[0m     validate_response_format(response_format)\n\u001B[1;32m--> 925\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    931\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maudio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    933\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    934\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    935\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    936\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    937\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodalities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreasoning_effort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    950\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    951\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    952\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    953\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    954\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    955\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    957\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    959\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweb_search_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    965\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\openai\\_base_client.py:1249\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1236\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1237\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1244\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1245\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1246\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1247\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1248\u001B[0m     )\n\u001B[1;32m-> 1249\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\é¡¹ç›®\\å¾®è°ƒ\\.venv\\lib\\site-packages\\openai\\_base_client.py:1004\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaising connection error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1004\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(request\u001B[38;5;241m=\u001B[39mrequest) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   1006\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[0;32m   1007\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHTTP Response: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1008\u001B[0m     request\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     response\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m   1013\u001B[0m )\n\u001B[0;32m   1014\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest_id: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx-request-id\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[1;31mAPIConnectionError\u001B[0m: Connection error."
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RGA + FiD é£æ ¼ æ£€ç´¢ ",
   "id": "bfd9d1f70008a178"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:38:25.316196Z",
     "start_time": "2025-07-16T06:38:25.302206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, faiss, numpy as np\n",
    "from typing import Dict\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ====== é…ç½® ======\n",
    "DASH_API_KEY = \"sk-0b8d48b85f1742829ef3032133375d3e\"\n",
    "EMBED_MODEL = \"text-embedding-v2\"\n",
    "LLM_MODEL = \"qwen2.5-72b-instruct\"\n",
    "EMBED_DIM = 256\n",
    "EMBEDDING   = DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=DASH_API_KEY)\n",
    "\n",
    "LLM       =  ChatOpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "                 api_key=DASH_API_KEY,\n",
    "                 model=LLM_MODEL, temperature=0.7)\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 150\n",
    "CHUNK_OVERLAP = 20\n",
    "TOP_K = 4\n",
    "EXPANSION_N = 3\n",
    "\n",
    "# ===== å¾®å‹è¯­æ–™åº“ =====\n",
    "RAW_TEXTS = [\n",
    "    \"æ‰“å°æœºåº”æ”¾ç½®åœ¨é€šé£è‰¯å¥½çš„ç¯å¢ƒä¸­ä»¥é¿å…è¿‡çƒ­ã€‚\",\n",
    "    \"ä½¿ç”¨æ‰“å°æœºå‰åº”æ£€æŸ¥ç”µç¼†è¿æ¥æ˜¯å¦ç‰¢å›ºã€‚\",\n",
    "    \"ç»´æŠ¤æ‰“å°æœºå‰ï¼Œè¯·ç¡®ä¿è®¾å¤‡æ–­ç”µä»¥é¿å…è§¦ç”µé£é™©ã€‚\",\n",
    "    \"èŠ‚èƒ½æ¨¡å¼å¯ä»¥å°†æ‰“å°æœºèƒ½è€—é™ä½æœ€å¤š30%ã€‚\",\n",
    "    \"ä½¿ç”¨åŸè£…è€—æå¯ä»¥æé«˜æ‰“å°è´¨é‡å’Œè®¾å¤‡å¯¿å‘½ã€‚\"\n",
    "]\n",
    "docs = [Document(page_content=t, metadata={\"id\": str(i)}) for i, t in enumerate(RAW_TEXTS)]\n",
    "\n",
    "# ===== åˆ‡åˆ†å¹¶æ„å»º chunk ç´¢å¼• =====\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = splitter.split_documents(docs)\n",
    "chunk_vecs = np.array(EMBEDDING.embed_documents([c.page_content for c in chunks]), dtype=\"float32\")\n",
    "faiss.normalize_L2(chunk_vecs)\n",
    "index = faiss.IndexFlatIP(chunk_vecs.shape[1])\n",
    "index.add(chunk_vecs)\n",
    "\n",
    "# mapping\n",
    "chunk2doc = chunks\n",
    "doc_id_map = {i: c for i, c in enumerate(chunks)}\n",
    "\n",
    "# ===== æŸ¥è¯¢æ”¹å†™å™¨ï¼ˆRGAï¼‰ =====\n",
    "query_expander = LLMChain(\n",
    "    llm=LLM,\n",
    "    prompt=PromptTemplate.from_template(\n",
    "        \"ä½ æ˜¯ä¸€åæŸ¥è¯¢å¢å¼ºåŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹æŸ¥è¯¢æ”¹å†™ä¸º {n} æ¡ä¸åŒæªè¾çš„è¡¨è¾¾ï¼Œæ¯è¡Œä¸€æ¡ï¼š\\nç”¨æˆ·æŸ¥è¯¢ï¼š{query}\\næ”¹å†™ï¼š\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ===== FiD é£æ ¼é—®ç­” Prompt =====\n",
    "fid_prompt = PromptTemplate.from_template(\n",
    "    \"æ ¹æ®ä»¥ä¸‹å¤šä¸ªå‚è€ƒå†…å®¹ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚\\n\\nå‚è€ƒä¿¡æ¯ï¼š\\n{context}\\n\\nç”¨æˆ·é—®é¢˜ï¼š{question}\\n\\nå›ç­”ï¼š\"\n",
    ")\n",
    "\n",
    "# ===== æ£€ç´¢å¢å¼ºå›ç­”æµç¨‹ =====\n",
    "def rga_fid_qa(query: str):\n",
    "    # (1) æ‰©å†™æŸ¥è¯¢\n",
    "    expanded = query_expander.run({\"query\": query, \"n\": EXPANSION_N})\n",
    "    queries = [query] + [q.strip(\" 123456.-\") for q in expanded.split(\"\\n\") if q.strip()]\n",
    "    print(\"ğŸ” æ‰©å†™æŸ¥è¯¢ï¼š\", queries)\n",
    "\n",
    "    # (2) åˆ†åˆ«æ£€ç´¢ top-k chunk\n",
    "    all_chunks = []\n",
    "    for q in queries:\n",
    "        q_vec = np.array(EMBEDDING.embed_query(q), dtype=\"float32\").reshape(1, -1)\n",
    "        faiss.normalize_L2(q_vec)\n",
    "        _, I = index.search(q_vec, TOP_K)\n",
    "        all_chunks.extend([doc_id_map[i] for i in I[0]])\n",
    "\n",
    "    # å»é‡\n",
    "    unique_chunks = {c.page_content for c in all_chunks}\n",
    "    context = \"\\n\".join(unique_chunks)\n",
    "\n",
    "    # (3) æ‹¼æ¥è¾“å…¥ -> FiD é£æ ¼ç”Ÿæˆ\n",
    "    final_prompt = fid_prompt.format(context=context, question=query)\n",
    "    answer = LLM.invoke(final_prompt).content\n",
    "    return answer\n",
    "\n",
    "# ===== è¿è¡Œç¤ºä¾‹ =====\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"å¦‚ä½•ä¿éšœæ‰“å°æœºä½¿ç”¨è¿‡ç¨‹çš„å®‰å…¨ï¼Ÿ\"\n",
    "    result = rga_fid_qa(q)\n",
    "    print(\"\\nğŸ§  å›ç­”ï¼š\", result)\n"
   ],
   "id": "5e26d1cb248057b0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RGA å‹ç¼©ç¨ å¯†æ£€ç´¢çš„å®šä¹‰",
   "id": "ec496c6239ee8e35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# ====== é…ç½® ======\n",
    "DASH_API_KEY = \"sk-0b8d48b85f1742829ef3032133375d3e\"\n",
    "EMBED_MODEL = \"text-embedding-v2\"\n",
    "LLM_MODEL = \"qwen2.5-72b-instruct\"\n",
    "\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBED_DIM = 256  # âœ… å‹ç¼©ç»´åº¦\n",
    "TOP_K = 5\n",
    "EXPANSION_N = 3\n",
    "\n",
    "# ====== å‘é‡æ¨¡å‹ ======\n",
    "embedding =  DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=DASH_API_KEY)\n",
    "llm = ChatOpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "                 api_key=DASH_API_KEY,\n",
    "                 model=LLM_MODEL, temperature=0.7)\n",
    "\n",
    "# ====== â‘  åŠ è½½æ•°æ®å¹¶æ„å»ºç´¢å¼• ======\n",
    "def load_and_index(doc_path: str):\n",
    "    loader = TextLoader(doc_path, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "    return vectorstore\n",
    "\n",
    "# ====== â‘¡ æ„å»ºæŸ¥è¯¢æ‰©å±•å™¨ï¼ˆRGAï¼‰======\n",
    "def build_query_expander():\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"ä½ æ˜¯ä¸€åæ£€ç´¢å¢å¼ºåŠ©æ‰‹ï¼Œè¯·å°†ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ï¼Œç”Ÿæˆ {n} æ¡ä¸åŒå…³é”®è¯æˆ–è¡¨è¾¾æ–¹å¼çš„æ‰©å±•æŸ¥è¯¢ï¼Œç”¨æ¢è¡Œåˆ†éš”ï¼š\n",
    "        ç”¨æˆ·æŸ¥è¯¢ï¼š{query}\n",
    "        æ‰©å±•æŸ¥è¯¢ï¼š\"\"\"\n",
    "    )\n",
    "    return LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# ====== â‘¢ å‹ç¼©ç¨ å¯†æ£€ç´¢ + æŸ¥è¯¢æ‰©å±• ======\n",
    "def rga_compressed_retrieve(query: str, vectorstore: FAISS, expander: LLMChain):\n",
    "    # (1) æ‰©å±•æŸ¥è¯¢\n",
    "    expanded = expander.run({\"query\": query, \"n\": EXPANSION_N})\n",
    "    queries = [query] + [q.strip(\" 123456.-\") for q in expanded.split(\"\\n\") if q.strip()]\n",
    "    print(\"ğŸ” æ‰©å±•æŸ¥è¯¢ï¼š\", queries)\n",
    "\n",
    "    # (2) å¤šæŸ¥è¯¢æ£€ç´¢\n",
    "    seen = set()\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        docs = vectorstore.similarity_search(q, k=TOP_K)\n",
    "        for d in docs:\n",
    "            if d.page_content not in seen:\n",
    "                results.append(d)\n",
    "                seen.add(d.page_content)\n",
    "    return results[:TOP_K]\n",
    "\n",
    "# ====== â‘£ ä¸»å‡½æ•° ======\n",
    "if __name__ == \"__main__\":\n",
    "    vs = load_and_index(\"æ›²é¢æ‰“å°æœºè¯´æ˜ä¹¦.txt\")  # æ›¿æ¢ä¸ºä½ çš„æ–‡æ¡£\n",
    "    expander = build_query_expander()\n",
    "    query = \"æ‰“å°æœºçš„èŠ‚èƒ½è®¾è®¡æœ‰å“ªäº›ï¼Ÿ\"\n",
    "\n",
    "    results = rga_compressed_retrieve(query, vs, expander)\n",
    "    print(f\"\\nğŸ¯ Top {len(results)} æ®µè½ï¼š\")\n",
    "    for i, d in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] {d.page_content[:200]}...\")\n"
   ],
   "id": "6ccd65c64c571396"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hybridâ€¯Dense + Metadata æ£€ç´¢\n",
    "###   ç‰¹ç‚¹ï¼š\n",
    "RGAï¼ˆRetrieval with Generated Augmentationï¼‰ï¼šæŸ¥è¯¢æ”¹å†™å¢å¼º\n",
    "\n",
    "Dense å‘é‡æ£€ç´¢ï¼šåŸºäº OpenAI æˆ– DashScope embedding\n",
    "\n",
    "Metadataï¼ˆå…ƒæ•°æ®ï¼‰è¿‡æ»¤ï¼šåŸºäºæ–‡æ¡£æ¥æºã€æ—¶é—´ã€ä½œè€…ç­‰æ¡ä»¶è¿‡æ»¤æ–‡æ¡£\n",
    "\n",
    "Hybrid æ£€ç´¢ï¼šç¨ å¯† + ç»“æ„ä¿¡æ¯ï¼ˆmetadataï¼‰åŒé€šé“ç­›é€‰"
   ],
   "id": "415227ce5a97c276"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# ====== é…ç½® ======\n",
    "DASH_API_KEY = \"sk-0b8d48b85f1742829ef3032133375d3e\"\n",
    "EMBED_MODEL = \"text-embedding-v2\"\n",
    "LLM_MODEL = \"qwen2.5-72b-instruct\"\n",
    "\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBED_DIM = 256  # âœ… å‹ç¼©ç»´åº¦\n",
    "TOP_K = 5\n",
    "EXPANSION_N = 3\n",
    "\n",
    "# ====== å‘é‡æ¨¡å‹ ======\n",
    "embedding =  DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=DASH_API_KEY)\n",
    "llm = ChatOpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "                 api_key=DASH_API_KEY,\n",
    "                 model=LLM_MODEL, temperature=0.7)\n",
    "\n",
    "# ========== â‘  åŠ è½½æ–‡æ¡£ ==========\n",
    "def load_documents(path: str, metadata_value: str):\n",
    "    loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    # åŠ å…¥å…ƒä¿¡æ¯ metadata\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"category\"] = metadata_value  # å¯æ‰©å±•å¤šä¸ªå­—æ®µ\n",
    "    return docs\n",
    "\n",
    "# ========== â‘¡ å»ºç´¢å¼• ==========\n",
    "def build_index(docs: List[Document]):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    return FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "# ========== â‘¢ æŸ¥è¯¢æ‰©å†™å™¨ ==========\n",
    "def build_expander():\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"ä½ æ˜¯ä¸€åæ£€ç´¢åŠ©æ‰‹ï¼Œæ‰©å†™ä¸‹åˆ—ç”¨æˆ·æŸ¥è¯¢ä¸º {n} æ¡ä¸åŒè¡¨è¾¾çš„æ£€ç´¢å¼ï¼Œç”¨æ¢è¡Œåˆ†éš”ï¼š\n",
    "        ç”¨æˆ·æŸ¥è¯¢ï¼š{query}\n",
    "        æ‰©å†™ï¼š\"\"\"\n",
    "    )\n",
    "    return LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# ========== â‘£ æ··åˆæ£€ç´¢å‡½æ•° ==========\n",
    "def rga_hybrid_search(query: str, index: FAISS, expander: LLMChain,\n",
    "                      metadata_filter: dict = None):\n",
    "    # 1. æŸ¥è¯¢æ‰©å†™\n",
    "    expansion = expander.run({\"query\": query, \"n\": EXPANSION_N})\n",
    "    queries = [query] + [q.strip(\" 123456.-\") for q in expansion.split(\"\\n\") if q.strip()]\n",
    "    print(\"ğŸ” æ‰©å±•æŸ¥è¯¢ï¼š\", queries)\n",
    "\n",
    "    # 2. æŸ¥è¯¢ + å…ƒä¿¡æ¯è¿‡æ»¤\n",
    "    matched_docs = []\n",
    "    seen = set()\n",
    "\n",
    "    for q in queries:\n",
    "        docs = index.similarity_search(q, k=TOP_K)\n",
    "        for doc in docs:\n",
    "            # âœ… metadata è¿‡æ»¤\n",
    "            if metadata_filter:\n",
    "                passed = all(doc.metadata.get(k) == v for k, v in metadata_filter.items())\n",
    "                if not passed:\n",
    "                    continue\n",
    "            if doc.page_content not in seen:\n",
    "                matched_docs.append(doc)\n",
    "                seen.add(doc.page_content)\n",
    "\n",
    "    return matched_docs[:TOP_K]\n",
    "\n",
    "# ========== â‘¤ ä¸»ç¨‹åº ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # æ¨¡æ‹ŸåŠ è½½å¤šç§ç±»å‹æ–‡æ¡£\n",
    "    safety_docs = load_documents(\"æ›²é¢æ‰“å°æœºè¯´æ˜ä¹¦.txt\", metadata_value=\"safety\")\n",
    "    usage_docs = load_documents(\"text.txt\", metadata_value=\"usage\")\n",
    "\n",
    "    all_docs = safety_docs + usage_docs\n",
    "    index = build_index(all_docs)\n",
    "    expander = build_expander()\n",
    "\n",
    "    user_query = \"æ‰“å°æœºå¯èƒ½æœ‰å“ªäº›å±é™©å› ç´ ï¼Ÿ\"\n",
    "    filtered_metadata = {\"category\": \"safety\"}  # âœ… æŒ‡å®šè¿‡æ»¤æ¡ä»¶\n",
    "\n",
    "    results = rga_hybrid_search(user_query, index, expander, filtered_metadata)\n",
    "\n",
    "    print(f\"\\nğŸ¯ Top {len(results)} ç»“æœï¼š\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] {doc.metadata} \\n{doc.page_content[:200]}...\")\n"
   ],
   "id": "71f83957ba897f86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAG-Fusion",
   "id": "cee826cd97a93d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "\n",
    "# Initialize OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Alternative: Use environment variable\n",
    "if openai.api_key is None:\n",
    "    raise Exception(\"No OpenAI API key found. Please set it as an environment variable or in main.py\")\n",
    "\n",
    "# Function to generate queries using OpenAI's ChatGPT\n",
    "def generate_queries_chatgpt(original_query):\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates multiple search queries based on a single input query.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate multiple search queries related to: {original_query}\"},\n",
    "            {\"role\": \"user\", \"content\": \"OUTPUT (4 queries):\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    generated_queries = response.choices[0][\"message\"][\"content\"].strip().split(\"\\n\")\n",
    "    return generated_queries\n",
    "\n",
    "# Mock function to simulate vector search, returning random scores\n",
    "def vector_search(query, all_documents):\n",
    "    available_docs = list(all_documents.keys())\n",
    "    random.shuffle(available_docs)\n",
    "    selected_docs = available_docs[:random.randint(2, 5)]\n",
    "    scores = {doc: round(random.uniform(0.7, 0.9), 2) for doc in selected_docs}\n",
    "    return {doc: score for doc, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "\n",
    "# Reciprocal Rank Fusion algorithm\n",
    "def reciprocal_rank_fusion(search_results_dict, k=60):\n",
    "    fused_scores = {}\n",
    "    print(\"Initial individual search result ranks:\")\n",
    "    for query, doc_scores in search_results_dict.items():\n",
    "        print(f\"For query '{query}': {doc_scores}\")\n",
    "        \n",
    "    for query, doc_scores in search_results_dict.items():\n",
    "        for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n",
    "            if doc not in fused_scores:\n",
    "                fused_scores[doc] = 0\n",
    "            previous_score = fused_scores[doc]\n",
    "            fused_scores[doc] += 1 / (rank + k)\n",
    "            print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n",
    "\n",
    "    reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "    print(\"Final reranked results:\", reranked_results)\n",
    "    return reranked_results\n",
    "\n",
    "# Dummy function to simulate generative output\n",
    "def generate_output(reranked_results, queries):\n",
    "    return f\"Final output based on {queries} and reranked documents: {list(reranked_results.keys())}\"\n",
    "\n",
    "\n",
    "# Predefined set of documents (usually these would be from your search database)\n",
    "all_documents = {\n",
    "    \"doc1\": \"Climate change and economic impact.\",\n",
    "    \"doc2\": \"Public health concerns due to climate change.\",\n",
    "    \"doc3\": \"Climate change: A social perspective.\",\n",
    "    \"doc4\": \"Technological solutions to climate change.\",\n",
    "    \"doc5\": \"Policy changes needed to combat climate change.\",\n",
    "    \"doc6\": \"Climate change and its impact on biodiversity.\",\n",
    "    \"doc7\": \"Climate change: The science and models.\",\n",
    "    \"doc8\": \"Global warming: A subset of climate change.\",\n",
    "    \"doc9\": \"How climate change affects daily weather.\",\n",
    "    \"doc10\": \"The history of climate change activism.\"\n",
    "}\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    original_query = \"impact of climate change\"\n",
    "    generated_queries = generate_queries_chatgpt(original_query)\n",
    "    \n",
    "    all_results = {}\n",
    "    for query in generated_queries:\n",
    "        search_results = vector_search(query, all_documents)\n",
    "        all_results[query] = search_results\n",
    "    \n",
    "    reranked_results = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    final_output = generate_output(reranked_results, generated_queries)\n",
    "    \n",
    "    print(final_output)"
   ],
   "id": "50550be37cbeebe3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
