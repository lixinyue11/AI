{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-16T06:06:08.021456Z",
     "start_time": "2025-07-16T06:06:08.003216Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "api_key = \"sk-07b0bbfd24bf4cb391cad5da8da05f6f\"\n",
    "base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "model_name = \"text-embedding-v3\"\n",
    "BATCH_SIZE  = 32\n",
    "TOP_K       = 3  \n",
    "\n",
    "# from langchain_community.embeddings import DashScopeEmbeddings\n",
    "# embeddings = DashScopeEmbeddings(\n",
    "#     model=\"text-embedding-v2\",\n",
    "#     dashscope_api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\")\n",
    "# llm = ChatOpenAI(base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "#                  api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\",\n",
    "#                  model=\"qwen2.5-72b-instruct\", temperature=0.7)\n",
    "# client_openai = OpenAI(\n",
    "#     api_key= \"sk-07b0bbfd24bf4cb391cad5da8da05f6f\",\n",
    "#     base_url= \"https://dashscope.aliyuncs.com/compatible-mode/v1\" \n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dual‑Encoder 检索",
   "id": "7add97fdc90a653c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:08:04.230238Z",
     "start_time": "2025-07-16T06:08:03.424007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "        和以下的区别，这里可以做成LLM生成答案，支持支持 IndexIVFFlat 带训练的倒排索引\n",
    "        quantizer = faiss.IndexFlatIP(dim)\n",
    "        index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "        if not index.is_trained:index.train(vectors)\n",
    "        index.add(vectors)\n",
    "'''\n",
    "''' 只做检索,使用 IndexFlatIP 或归一化后做余弦相似度检索,可扩展为多文档结构\n",
    " faiss.normalize_L2(doc_vectors)\n",
    "dim = doc_vectors.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)           # 内积，归一化后≈余弦\n",
    "index.add(doc_vectors)\n",
    " '''\n",
    "api_key = \"sk-07b0bbfd24bf4cb391cad5da8da05f6f\"\n",
    "base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "model_name = \"text-embedding-v3\"\n",
    "BATCH_SIZE  = 32\n",
    "TOP_K       = 3\n",
    "embedding_client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=base_url\n",
    "    )\n",
    "def embed_texts(texts):\n",
    "    resp = embedding_client.embeddings.create(\n",
    "            model=model_name,\n",
    "            input=batch,\n",
    "            #dimensions=dimensions,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "    # OpenAI v1 返回顺序与输入保持一致\n",
    "    return np.array([d.embedding for d in resp.data], dtype=\"float32\")\n",
    "\n",
    "documents = [\n",
    "    \"RAG 是 Retrieval‑Augmented Generation 的缩写。\",\n",
    "    \"Dual‑Encoder 采用查询和文档两个塔独立编码。\",\n",
    "    \"OpenAI 提供 text‑embedding‑3 系列模型。\",\n",
    "    \"在图谱检索中常用图数据库 Neo4j。\",\n",
    "]\n",
    "doc_vectors = []\n",
    "for i in tqdm(range(0, len(documents), BATCH_SIZE)):\n",
    "    batch = documents[i:i+BATCH_SIZE]\n",
    "    doc_vectors.append(embed_texts(batch))\n",
    "\n",
    "doc_vectors = np.vstack(doc_vectors)\n",
    "\n",
    "# ── 2.3 可选：余弦检索 → 先做 L2 归一化\n",
    "faiss.normalize_L2(doc_vectors)\n",
    "\n",
    "# ── 2.4 建立 FAISS 索引（余弦 = 归一化后 L2）\n",
    "dim = doc_vectors.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)           # 内积，归一化后≈余弦\n",
    "index.add(doc_vectors)\n",
    "def search(query, k=TOP_K):\n",
    "    # 3.1 编码查询\n",
    "    q_vec = embed_texts([query])\n",
    "    faiss.normalize_L2(q_vec)\n",
    "\n",
    "    # 3.2 ANN 检索\n",
    "    scores, indices = index.search(q_vec, k)\n",
    "    return [(documents[i], float(scores[0][j])) for j, i in enumerate(indices[0])]\n",
    "\n",
    "# 测试\n",
    "query = \"如何用双塔做语义检索？\"\n",
    "for rank, (text, score) in enumerate(search(query), 1):\n",
    "    print(f\"{rank}. {text}  (similarity={score:.4f})\")"
   ],
   "id": "6c91d39b081e5ad6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. RAG 是 Retrieval‑Augmented Generation 的缩写。  (similarity=1.0000)\n",
      "2. Dual‑Encoder 采用查询和文档两个塔独立编码。  (similarity=0.4616)\n",
      "3. 在图谱检索中常用图数据库 Neo4j。  (similarity=0.4595)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi‑vector 检索\n",
    "### 为每一个文档块创建子分块， 基于子分块和原始文档块进行匹配， 返回原始文档块。\n",
    "### 为每一个文档块创建摘要(Summary)， 基于摘要和原始文档块进行匹配， 返回原始文档块。\n",
    "### 为每一个文档块创建假设查询(Hypothetical Questions)， 基于假设查询和原始文档块进行匹配， 返回原始文档块。"
   ],
   "id": "e8ff81378177309b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:31:42.245706Z",
     "start_time": "2025-07-16T06:31:42.238505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 引入uuid库，用于生成唯一标识符\n",
    "import uuid\n",
    "from langchain.retrievers import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "loaders = [TextLoader('曲面打印机说明书.txt', encoding='utf-8') ,TextLoader('text.txt', encoding='utf-8') ]\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\",\n",
    "    dashscope_api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\")\n",
    "llm = ChatOpenAI(base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "                 api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\",\n",
    "                 model=\"qwen2.5-72b-instruct\", temperature=0.7)\n",
    "# 创建一个RecursiveCharacterTextSplitter对象，用于分割文本\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000)\n",
    "# 使用TextLoader对象加载文本文件\n",
    "# docs = loader.load()\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "# 使用RecursiveCharacterTextSplitter对象分割文档\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# 定义一个字典，键为\"doc\"，值为一个函数，该函数返回文档的内容\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # 使用ChatPromptTemplate类的from_template方法创建一个聊天提示模板\n",
    "    | ChatPromptTemplate.from_template(\"总结以下文档:\\n\\n{doc}\")\n",
    "    # 创建一个ChatOpenAI对象，最大重试次数为0\n",
    "    |llm\n",
    "    # 创建一个StrOutputParser对象\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# 使用chain对象的batch方法批量处理文档，最大并发数为5\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# 创建一个Chroma对象，集合名为\"summaries\"，嵌入函数为OpenAIEmbeddings()\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=embeddings)\n",
    "# 创建一个InMemoryByteStore对象，用于存储字节数据\n",
    "store = InMemoryByteStore()\n",
    "# 定义一个字符串，值为\"doc_id\"\n",
    "id_key = \"doc_id\"\n",
    "# 创建一个MultiVectorRetriever对象\n",
    "retriever = MultiVectorRetriever(\n",
    "    # 设置向量存储为vectorstore对象\n",
    "    vectorstore=vectorstore,\n",
    "    # 设置字节存储为store对象\n",
    "    byte_store=store,\n",
    "    # 设置id键为\"id_key\"\n",
    "    id_key=id_key,\n",
    ")\n",
    "# 为每个文档生成一个唯一的ID\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "# 创建一个Document对象，页面内容为s，元数据为{id_key: doc_ids[i]}\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    # 对summaries中的每一个元素和它的索引进行迭代\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "# 使用retriever对象的vectorstore属性的add_documents方法添加文档\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "# 使用retriever对象的docstore属性的mset方法设置文档\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "# 使用vectorstore对象的similarity_search方法搜索与\"如何写文章\"相似的文档\n",
    "sub_docs = vectorstore.similarity_search(\"张三\")\n",
    "# 获取搜索结果的第一个文档\n",
    "print(sub_docs[0] )\n",
    "# 使用retriever对象的get_relevant_documents方法获取与\"如何写文章\"相关的文档\n",
    "retrieved_docs = retriever.get_relevant_documents(\"张三\")\n",
    "# 获取搜索结果的第一个文档的内容\n",
    "print(retrieved_docs[0].page_content  )\n"
   ],
   "id": "6d0c5dab1db61ed6",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## RGA QUERY改写\n",
    "### 通过对原始查询进行语义扩展或改写后，提升召回效果，适用于 Dense 检索、多向量检索、RAG 等场景"
   ],
   "id": "b42bdfe2b6c5faae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:31:54.628014Z",
     "start_time": "2025-07-16T06:31:54.613499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "# ===================== 配置区域 =====================\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\",\n",
    "    dashscope_api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\")\n",
    "llm = ChatOpenAI(base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "                 api_key=\"sk-0b8d48b85f1742829ef3032133375d3e\",\n",
    "                 model=\"qwen2.5-72b-instruct\", temperature=0.7)\n",
    "client_openai = OpenAI(\n",
    "    api_key= \"sk-07b0bbfd24bf4cb391cad5da8da05f6f\",\n",
    "    base_url= \"https://dashscope.aliyuncs.com/compatible-mode/v1\" \n",
    ")\n",
    "EXPANSION_NUM = 3   # RGA生成几个改写查询\n",
    "TOP_K = 4           # 每个子查询召回K个文档\n",
    "\n",
    "# ===================== 加载文档并构建索引 =====================\n",
    "def build_faiss_index(filepath):\n",
    "    loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "   \n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# ===================== 构建RGA查询扩充链 =====================\n",
    "def build_query_expansion_chain():\n",
    "   # llm = ChatOpenAI(model=client_openai, temperature=0.7)\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "你是一个检索增强助手，请基于用户的原始查询，生成 {num} 个不同角度但相关的检索查询，增强语义多样性。\n",
    "\n",
    "原始查询：{query}\n",
    "\n",
    "请输出一组改写查询：\n",
    "\"\"\")\n",
    "    return LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# ===================== 多查询检索并合并结果 =====================\n",
    "def rga_retrieve(query, vectorstore, expander, top_k=4):\n",
    "    # 1. RGA生成改写查询\n",
    "    expansion_prompt = expander.run({\"query\": query, \"num\": EXPANSION_NUM})\n",
    "    reformulated_queries = [line.strip(\" 123456.-\").strip() \n",
    "                            for line in expansion_prompt.strip().split(\"\\n\") if line.strip()]\n",
    "    \n",
    "    print(f\"\\n🧠 原始查询: {query}\")\n",
    "    print(\"🔁 改写查询:\")\n",
    "    for i, rq in enumerate(reformulated_queries):\n",
    "        print(f\"  [{i+1}] {rq}\")\n",
    "\n",
    "    # 2. 每个子查询分别检索\n",
    "    all_results = []\n",
    "    for rq in reformulated_queries:\n",
    "        results = vectorstore.similarity_search(rq, k=top_k)\n",
    "        all_results.extend(results)\n",
    "\n",
    "    # 3. 去重（可选：再排序）\n",
    "    seen = set()\n",
    "    unique_results = []\n",
    "    for d in all_results:\n",
    "        if d.page_content not in seen:\n",
    "            seen.add(d.page_content)\n",
    "            unique_results.append(d)\n",
    "\n",
    "    return unique_results[:top_k * EXPANSION_NUM]\n",
    "\n",
    "# ===================== 主流程 =====================\n",
    "if __name__ == \"__main__\":\n",
    "    vs = build_faiss_index(\"曲面打印机说明书.txt\")\n",
    "    expander = build_query_expansion_chain()\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\n请输入查询（空退出）：\").strip()\n",
    "        if not query:\n",
    "            break\n",
    "        results = rga_retrieve(query, vs, expander)\n",
    "\n",
    "        print(f\"\\n📚 共返回 {len(results)} 条结果：\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"\\n[{i+1}] {doc.metadata.get('source', '')}\")\n",
    "            print(doc.page_content[:200] + \"…\")\n"
   ],
   "id": "a02fa3c386434767",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RGA 层级稠密检索代码",
   "id": "9585e52fb53fa9a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:42:08.813027Z",
     "start_time": "2025-07-16T09:42:03.944395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, faiss, numpy as np\n",
    "from typing import Dict\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ====== 配置 ======\n",
    "DASH_API_KEY = \"sk-0b8d48b85f1742829ef3032133375d3e\"\n",
    "EMBED_MODEL = \"text-embedding-v2\"\n",
    "LLM_MODEL = \"qwen2.5-72b-instruct\"\n",
    "EMBED_DIM = 256\n",
    "DOC_EMB   = DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=DASH_API_KEY)\n",
    "CHUNK_EMB = DOC_EMB\n",
    "LLM       =  ChatOpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "                 api_key=DASH_API_KEY,\n",
    "                 model=LLM_MODEL, temperature=0.7)\n",
    "L1, L2 = 3, 4                     # doc / chunk top‑k\n",
    "EXP_N   = 3                       # RGA 改写数\n",
    "\n",
    "\n",
    "SPLITTER= RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=20)\n",
    "\n",
    "# ====== 1. 自带微型语料 ======\n",
    "RAW = [\n",
    "    \"打印机应在通风环境下使用，防止设备过热并降低火灾风险。\",\n",
    "    \"维修打印机前必须断电，以避免触电和机械伤害。\",\n",
    "    \"节能模式可让打印机功耗降低 30%。\"\n",
    "]\n",
    "docs   = [Document(page_content=t, metadata={\"id\": f\"d{i}\"}) for i, t in enumerate(RAW)]\n",
    "chunks = SPLITTER.split_documents(docs)\n",
    "\n",
    "# ====== 2. 建双索引 ======\n",
    "doc_vec = np.array(DOC_EMB.embed_documents([d.page_content for d in docs]), dtype=\"float32\")\n",
    "chk_vec = np.array(CHUNK_EMB.embed_documents([c.page_content for c in chunks]), dtype=\"float32\")\n",
    "for v in (doc_vec, chk_vec): faiss.normalize_L2(v)\n",
    "dim = doc_vec.shape[1]          # ← 自动抓取真实维度（应为 1536）\n",
    "doc_idx = faiss.IndexFlatIP(dim)\n",
    "doc_idx.add(doc_vec)\n",
    "\n",
    "chk_idx = faiss.IndexFlatIP(dim)\n",
    "chk_idx.add(chk_vec)\n",
    "\n",
    "chk2doc = [c.metadata[\"id\"] for c in chunks]\n",
    "id2doc  = {d.metadata[\"id\"]: d for d in docs}\n",
    "\n",
    "# ====== 3. RGA 扩写器 ======\n",
    "expander = LLMChain(\n",
    "    llm=LLM,\n",
    "    prompt=PromptTemplate.from_template(\n",
    "        \"给用户查询扩写 {n} 个不同表达，每行一条：\\n查询：{q}\\n扩写：\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ====== 4. 层级检索函数 ======\n",
    "def search(q: str):\n",
    "    exps = expander.run({\"q\": q, \"n\": EXP_N})\n",
    "    queries = [q] + [l.strip(\" 123456.-\") for l in exps.split(\"\\n\") if l.strip()]\n",
    "    print(\"🔍 扩展查询:\", queries)\n",
    "\n",
    "    # 文档级\n",
    "    qv = np.array([DOC_EMB.embed_query(x) for x in queries], dtype=\"float32\"); faiss.normalize_L2(qv)\n",
    "    _, I = doc_idx.search(qv, L1)\n",
    "    cand_ids = {docs[i].metadata[\"id\"] for i in I.flatten()}\n",
    "\n",
    "    # chunk 级\n",
    "    qv2 = np.array([CHUNK_EMB.embed_query(x) for x in queries], dtype=\"float32\"); faiss.normalize_L2(qv2)\n",
    "    D, I = chk_idx.search(qv2, L2)\n",
    "\n",
    "    scores: Dict[str, float] = {}\n",
    "    for ds, is_ in zip(D, I):\n",
    "        for s, idx in zip(ds, is_):\n",
    "            did = chk2doc[idx]\n",
    "            if did in cand_ids:\n",
    "                scores[did] = max(scores.get(did, -1), s)\n",
    "\n",
    "    return sorted([(id2doc[k], v) for k, v in scores.items()], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ====== 5. Demo 运行 ======\n",
    "for doc, sc in search(\"打印机安全隐患\")[:3]:\n",
    "    print(f\"\\nScore={sc:.3f} | {doc.page_content}\")\n"
   ],
   "id": "13a98855839ac3f1",
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mConnectError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:72\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[1;34m()\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 72\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:236\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[1;32m--> 236\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[1;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[1;32m--> 236\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[0;32m    242\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpcore\\_sync\\http_proxy.py:288\u001B[0m, in \u001B[0;36mTunnelHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    282\u001B[0m connect_request \u001B[38;5;241m=\u001B[39m Request(\n\u001B[0;32m    283\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCONNECT\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    284\u001B[0m     url\u001B[38;5;241m=\u001B[39mconnect_url,\n\u001B[0;32m    285\u001B[0m     headers\u001B[38;5;241m=\u001B[39mconnect_headers,\n\u001B[0;32m    286\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    287\u001B[0m )\n\u001B[1;32m--> 288\u001B[0m connect_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconnect_request\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connect_response\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m200\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m connect_response\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m299\u001B[39m:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection\u001B[38;5;241m.\u001B[39mhandle_request(request)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m     ssl_object \u001B[38;5;241m=\u001B[39m stream\u001B[38;5;241m.\u001B[39mget_extra_info(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mssl_object\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpcore\\_sync\\connection.py:156\u001B[0m, in \u001B[0;36mHTTPConnection._connect\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart_tls\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m--> 156\u001B[0m     stream \u001B[38;5;241m=\u001B[39m stream\u001B[38;5;241m.\u001B[39mstart_tls(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    157\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m stream\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpcore\\_backends\\sync.py:154\u001B[0m, in \u001B[0;36mSyncStream.start_tls\u001B[1;34m(self, ssl_context, server_hostname, timeout)\u001B[0m\n\u001B[0;32m    150\u001B[0m exc_map: ExceptionMapping \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    151\u001B[0m     socket\u001B[38;5;241m.\u001B[39mtimeout: ConnectTimeout,\n\u001B[0;32m    152\u001B[0m     \u001B[38;5;167;01mOSError\u001B[39;00m: ConnectError,\n\u001B[0;32m    153\u001B[0m }\n\u001B[1;32m--> 154\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\python310\\lib\\contextlib.py:153\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 153\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpcore\\_exceptions.py:14\u001B[0m, in \u001B[0;36mmap_exceptions\u001B[1;34m(map)\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[1;32m---> 14\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mConnectError\u001B[0m: EOF occurred in violation of protocol (_ssl.c:997)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mConnectError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\openai\\_base_client.py:972\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 972\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39msend(\n\u001B[0;32m    973\u001B[0m         request,\n\u001B[0;32m    974\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_stream_response_body(request\u001B[38;5;241m=\u001B[39mrequest),\n\u001B[0;32m    975\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    976\u001B[0m     )\n\u001B[0;32m    977\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpx\\_client.py:926\u001B[0m, in \u001B[0;36mClient.send\u001B[1;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[0;32m    924\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[1;32m--> 926\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    931\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpx\\_client.py:954\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[1;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[0;32m    953\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 954\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    955\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    957\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    959\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpx\\_client.py:991\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[1;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[0;32m    989\u001B[0m     hook(request)\n\u001B[1;32m--> 991\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    992\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpx\\_client.py:1027\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m   1026\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[1;32m-> 1027\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:235\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    223\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[0;32m    224\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m    225\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    233\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[0;32m    234\u001B[0m )\n\u001B[1;32m--> 235\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m    236\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool\u001B[38;5;241m.\u001B[39mhandle_request(req)\n",
      "File \u001B[1;32mD:\\python310\\lib\\contextlib.py:153\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[1;34m(self, typ, value, traceback)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 153\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\httpx\\_transports\\default.py:89\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[1;34m()\u001B[0m\n\u001B[0;32m     88\u001B[0m message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[1;32m---> 89\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[1;31mConnectError\u001B[0m: EOF occurred in violation of protocol (_ssl.c:997)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mAPIConnectionError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 121\u001B[0m\n\u001B[0;32m    118\u001B[0m expander \u001B[38;5;241m=\u001B[39m build_query_expander()\n\u001B[0;32m    119\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m打印机的安全性设计\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 121\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mhierarchical_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdoc_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdoc_path_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk2doc_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpander\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m5\u001B[39m]\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m🎯 Top 结果：\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (doc, score) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(results, \u001B[38;5;241m1\u001B[39m):\n",
      "Cell \u001B[1;32mIn[25], line 82\u001B[0m, in \u001B[0;36mhierarchical_search\u001B[1;34m(query, doc_index, chunk_index, doc_path_map, chunk2doc_path, chunks, query_expander)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mhierarchical_search\u001B[39m(query: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m     75\u001B[0m                         doc_index,\n\u001B[0;32m     76\u001B[0m                         chunk_index,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     80\u001B[0m                         query_expander):\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;66;03m# (1) RGA 查询扩展\u001B[39;00m\n\u001B[1;32m---> 82\u001B[0m     expansion \u001B[38;5;241m=\u001B[39m \u001B[43mquery_expander\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquery\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mEXPANSION_N\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     83\u001B[0m     queries \u001B[38;5;241m=\u001B[39m [query] \u001B[38;5;241m+\u001B[39m [q\u001B[38;5;241m.\u001B[39mstrip(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m 123456.-\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m expansion\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m q\u001B[38;5;241m.\u001B[39mstrip()]\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m🔍 扩展查询：\u001B[39m\u001B[38;5;124m\"\u001B[39m, queries)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    187\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    188\u001B[0m     emit_warning()\n\u001B[1;32m--> 189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:603\u001B[0m, in \u001B[0;36mChain.run\u001B[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[0;32m    601\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    602\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 603\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[0;32m    604\u001B[0m         _output_key\n\u001B[0;32m    605\u001B[0m     ]\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    609\u001B[0m         _output_key\n\u001B[0;32m    610\u001B[0m     ]\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    187\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    188\u001B[0m     emit_warning()\n\u001B[1;32m--> 189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:386\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \n\u001B[0;32m    356\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    379\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[0;32m    381\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[0;32m    384\u001B[0m }\n\u001B[1;32m--> 386\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:167\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    166\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    168\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain\\chains\\base.py:157\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[0;32m    156\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 157\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    160\u001B[0m     )\n\u001B[0;32m    162\u001B[0m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    163\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[0;32m    164\u001B[0m     )\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py:127\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_call\u001B[39m(\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    124\u001B[0m     inputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    125\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    126\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m--> 127\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain\\chains\\llm.py:139\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[1;34m(self, input_list, run_manager)\u001B[0m\n\u001B[0;32m    137\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[1;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    140\u001B[0m         prompts,\n\u001B[0;32m    141\u001B[0m         stop,\n\u001B[0;32m    142\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    143\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs,\n\u001B[0;32m    144\u001B[0m     )\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[0;32m    147\u001B[0m         cast(\u001B[38;5;28mlist\u001B[39m, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[0;32m    148\u001B[0m     )\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    949\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    950\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    954\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    955\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    956\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 957\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[0;32m    774\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    775\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 776\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    777\u001B[0m                 m,\n\u001B[0;32m    778\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    779\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    780\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    781\u001B[0m             )\n\u001B[0;32m    782\u001B[0m         )\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1020\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1022\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m   1023\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m   1024\u001B[0m     )\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1026\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1071\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1069\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response\u001B[38;5;241m.\u001B[39mheaders)}\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1071\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n\u001B[0;32m   1072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response, generation_info)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\openai\\_utils\\_utils.py:287\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    285\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    882\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    883\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    884\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    922\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    923\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m    924\u001B[0m     validate_response_format(response_format)\n\u001B[1;32m--> 925\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    931\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maudio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    933\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    934\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    935\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    936\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    937\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodalities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreasoning_effort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    950\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    951\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    952\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    953\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    954\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    955\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    957\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    959\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweb_search_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    965\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\openai\\_base_client.py:1249\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1236\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1237\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1244\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1245\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1246\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1247\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1248\u001B[0m     )\n\u001B[1;32m-> 1249\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\项目\\微调\\.venv\\lib\\site-packages\\openai\\_base_client.py:1004\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1001\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaising connection error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1004\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(request\u001B[38;5;241m=\u001B[39mrequest) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   1006\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[0;32m   1007\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHTTP Response: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   1008\u001B[0m     request\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     response\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[0;32m   1013\u001B[0m )\n\u001B[0;32m   1014\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest_id: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx-request-id\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[1;31mAPIConnectionError\u001B[0m: Connection error."
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RGA + FiD 风格 检索 ",
   "id": "bfd9d1f70008a178"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T06:38:25.316196Z",
     "start_time": "2025-07-16T06:38:25.302206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, faiss, numpy as np\n",
    "from typing import Dict\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ====== 配置 ======\n",
    "DASH_API_KEY = \"sk-0b8d48b85f1742829ef3032133375d3e\"\n",
    "EMBED_MODEL = \"text-embedding-v2\"\n",
    "LLM_MODEL = \"qwen2.5-72b-instruct\"\n",
    "EMBED_DIM = 256\n",
    "EMBEDDING   = DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=DASH_API_KEY)\n",
    "\n",
    "LLM       =  ChatOpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "                 api_key=DASH_API_KEY,\n",
    "                 model=LLM_MODEL, temperature=0.7)\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 150\n",
    "CHUNK_OVERLAP = 20\n",
    "TOP_K = 4\n",
    "EXPANSION_N = 3\n",
    "\n",
    "# ===== 微型语料库 =====\n",
    "RAW_TEXTS = [\n",
    "    \"打印机应放置在通风良好的环境中以避免过热。\",\n",
    "    \"使用打印机前应检查电缆连接是否牢固。\",\n",
    "    \"维护打印机前，请确保设备断电以避免触电风险。\",\n",
    "    \"节能模式可以将打印机能耗降低最多30%。\",\n",
    "    \"使用原装耗材可以提高打印质量和设备寿命。\"\n",
    "]\n",
    "docs = [Document(page_content=t, metadata={\"id\": str(i)}) for i, t in enumerate(RAW_TEXTS)]\n",
    "\n",
    "# ===== 切分并构建 chunk 索引 =====\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "chunks = splitter.split_documents(docs)\n",
    "chunk_vecs = np.array(EMBEDDING.embed_documents([c.page_content for c in chunks]), dtype=\"float32\")\n",
    "faiss.normalize_L2(chunk_vecs)\n",
    "index = faiss.IndexFlatIP(chunk_vecs.shape[1])\n",
    "index.add(chunk_vecs)\n",
    "\n",
    "# mapping\n",
    "chunk2doc = chunks\n",
    "doc_id_map = {i: c for i, c in enumerate(chunks)}\n",
    "\n",
    "# ===== 查询改写器（RGA） =====\n",
    "query_expander = LLMChain(\n",
    "    llm=LLM,\n",
    "    prompt=PromptTemplate.from_template(\n",
    "        \"你是一名查询增强助手。请将以下查询改写为 {n} 条不同措辞的表达，每行一条：\\n用户查询：{query}\\n改写：\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# ===== FiD 风格问答 Prompt =====\n",
    "fid_prompt = PromptTemplate.from_template(\n",
    "    \"根据以下多个参考内容，回答用户问题。\\n\\n参考信息：\\n{context}\\n\\n用户问题：{question}\\n\\n回答：\"\n",
    ")\n",
    "\n",
    "# ===== 检索增强回答流程 =====\n",
    "def rga_fid_qa(query: str):\n",
    "    # (1) 扩写查询\n",
    "    expanded = query_expander.run({\"query\": query, \"n\": EXPANSION_N})\n",
    "    queries = [query] + [q.strip(\" 123456.-\") for q in expanded.split(\"\\n\") if q.strip()]\n",
    "    print(\"🔍 扩写查询：\", queries)\n",
    "\n",
    "    # (2) 分别检索 top-k chunk\n",
    "    all_chunks = []\n",
    "    for q in queries:\n",
    "        q_vec = np.array(EMBEDDING.embed_query(q), dtype=\"float32\").reshape(1, -1)\n",
    "        faiss.normalize_L2(q_vec)\n",
    "        _, I = index.search(q_vec, TOP_K)\n",
    "        all_chunks.extend([doc_id_map[i] for i in I[0]])\n",
    "\n",
    "    # 去重\n",
    "    unique_chunks = {c.page_content for c in all_chunks}\n",
    "    context = \"\\n\".join(unique_chunks)\n",
    "\n",
    "    # (3) 拼接输入 -> FiD 风格生成\n",
    "    final_prompt = fid_prompt.format(context=context, question=query)\n",
    "    answer = LLM.invoke(final_prompt).content\n",
    "    return answer\n",
    "\n",
    "# ===== 运行示例 =====\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"如何保障打印机使用过程的安全？\"\n",
    "    result = rga_fid_qa(q)\n",
    "    print(\"\\n🧠 回答：\", result)\n"
   ],
   "id": "5e26d1cb248057b0",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RGA 压缩稠密检索的定义",
   "id": "ec496c6239ee8e35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# ====== 配置 ======\n",
    "DASH_API_KEY = \"sk-0b8d48b85f1742829ef3032133375d3e\"\n",
    "EMBED_MODEL = \"text-embedding-v2\"\n",
    "LLM_MODEL = \"qwen2.5-72b-instruct\"\n",
    "\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBED_DIM = 256  # ✅ 压缩维度\n",
    "TOP_K = 5\n",
    "EXPANSION_N = 3\n",
    "\n",
    "# ====== 向量模型 ======\n",
    "embedding =  DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=DASH_API_KEY)\n",
    "llm = ChatOpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "                 api_key=DASH_API_KEY,\n",
    "                 model=LLM_MODEL, temperature=0.7)\n",
    "\n",
    "# ====== ① 加载数据并构建索引 ======\n",
    "def load_and_index(doc_path: str):\n",
    "    loader = TextLoader(doc_path, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "    return vectorstore\n",
    "\n",
    "# ====== ② 构建查询扩展器（RGA）======\n",
    "def build_query_expander():\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"你是一名检索增强助手，请将用户输入的查询，生成 {n} 条不同关键词或表达方式的扩展查询，用换行分隔：\n",
    "        用户查询：{query}\n",
    "        扩展查询：\"\"\"\n",
    "    )\n",
    "    return LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# ====== ③ 压缩稠密检索 + 查询扩展 ======\n",
    "def rga_compressed_retrieve(query: str, vectorstore: FAISS, expander: LLMChain):\n",
    "    # (1) 扩展查询\n",
    "    expanded = expander.run({\"query\": query, \"n\": EXPANSION_N})\n",
    "    queries = [query] + [q.strip(\" 123456.-\") for q in expanded.split(\"\\n\") if q.strip()]\n",
    "    print(\"🔍 扩展查询：\", queries)\n",
    "\n",
    "    # (2) 多查询检索\n",
    "    seen = set()\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        docs = vectorstore.similarity_search(q, k=TOP_K)\n",
    "        for d in docs:\n",
    "            if d.page_content not in seen:\n",
    "                results.append(d)\n",
    "                seen.add(d.page_content)\n",
    "    return results[:TOP_K]\n",
    "\n",
    "# ====== ④ 主函数 ======\n",
    "if __name__ == \"__main__\":\n",
    "    vs = load_and_index(\"曲面打印机说明书.txt\")  # 替换为你的文档\n",
    "    expander = build_query_expander()\n",
    "    query = \"打印机的节能设计有哪些？\"\n",
    "\n",
    "    results = rga_compressed_retrieve(query, vs, expander)\n",
    "    print(f\"\\n🎯 Top {len(results)} 段落：\")\n",
    "    for i, d in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] {d.page_content[:200]}...\")\n"
   ],
   "id": "6ccd65c64c571396"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hybrid Dense + Metadata 检索\n",
    "###   特点：\n",
    "RGA（Retrieval with Generated Augmentation）：查询改写增强\n",
    "\n",
    "Dense 向量检索：基于 OpenAI 或 DashScope embedding\n",
    "\n",
    "Metadata（元数据）过滤：基于文档来源、时间、作者等条件过滤文档\n",
    "\n",
    "Hybrid 检索：稠密 + 结构信息（metadata）双通道筛选"
   ],
   "id": "415227ce5a97c276"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# ====== 配置 ======\n",
    "DASH_API_KEY = \"sk-0b8d48b85f1742829ef3032133375d3e\"\n",
    "EMBED_MODEL = \"text-embedding-v2\"\n",
    "LLM_MODEL = \"qwen2.5-72b-instruct\"\n",
    "\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 50\n",
    "EMBED_DIM = 256  # ✅ 压缩维度\n",
    "TOP_K = 5\n",
    "EXPANSION_N = 3\n",
    "\n",
    "# ====== 向量模型 ======\n",
    "embedding =  DashScopeEmbeddings(model=EMBED_MODEL, dashscope_api_key=DASH_API_KEY)\n",
    "llm = ChatOpenAI(base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "                 api_key=DASH_API_KEY,\n",
    "                 model=LLM_MODEL, temperature=0.7)\n",
    "\n",
    "# ========== ① 加载文档 ==========\n",
    "def load_documents(path: str, metadata_value: str):\n",
    "    loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    # 加入元信息 metadata\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"category\"] = metadata_value  # 可扩展多个字段\n",
    "    return docs\n",
    "\n",
    "# ========== ② 建索引 ==========\n",
    "def build_index(docs: List[Document]):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    return FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "# ========== ③ 查询扩写器 ==========\n",
    "def build_expander():\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"你是一名检索助手，扩写下列用户查询为 {n} 条不同表达的检索式，用换行分隔：\n",
    "        用户查询：{query}\n",
    "        扩写：\"\"\"\n",
    "    )\n",
    "    return LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# ========== ④ 混合检索函数 ==========\n",
    "def rga_hybrid_search(query: str, index: FAISS, expander: LLMChain,\n",
    "                      metadata_filter: dict = None):\n",
    "    # 1. 查询扩写\n",
    "    expansion = expander.run({\"query\": query, \"n\": EXPANSION_N})\n",
    "    queries = [query] + [q.strip(\" 123456.-\") for q in expansion.split(\"\\n\") if q.strip()]\n",
    "    print(\"🔍 扩展查询：\", queries)\n",
    "\n",
    "    # 2. 查询 + 元信息过滤\n",
    "    matched_docs = []\n",
    "    seen = set()\n",
    "\n",
    "    for q in queries:\n",
    "        docs = index.similarity_search(q, k=TOP_K)\n",
    "        for doc in docs:\n",
    "            # ✅ metadata 过滤\n",
    "            if metadata_filter:\n",
    "                passed = all(doc.metadata.get(k) == v for k, v in metadata_filter.items())\n",
    "                if not passed:\n",
    "                    continue\n",
    "            if doc.page_content not in seen:\n",
    "                matched_docs.append(doc)\n",
    "                seen.add(doc.page_content)\n",
    "\n",
    "    return matched_docs[:TOP_K]\n",
    "\n",
    "# ========== ⑤ 主程序 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # 模拟加载多种类型文档\n",
    "    safety_docs = load_documents(\"曲面打印机说明书.txt\", metadata_value=\"safety\")\n",
    "    usage_docs = load_documents(\"text.txt\", metadata_value=\"usage\")\n",
    "\n",
    "    all_docs = safety_docs + usage_docs\n",
    "    index = build_index(all_docs)\n",
    "    expander = build_expander()\n",
    "\n",
    "    user_query = \"打印机可能有哪些危险因素？\"\n",
    "    filtered_metadata = {\"category\": \"safety\"}  # ✅ 指定过滤条件\n",
    "\n",
    "    results = rga_hybrid_search(user_query, index, expander, filtered_metadata)\n",
    "\n",
    "    print(f\"\\n🎯 Top {len(results)} 结果：\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] {doc.metadata} \\n{doc.page_content[:200]}...\")\n"
   ],
   "id": "71f83957ba897f86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAG-Fusion",
   "id": "cee826cd97a93d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "\n",
    "# Initialize OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Alternative: Use environment variable\n",
    "if openai.api_key is None:\n",
    "    raise Exception(\"No OpenAI API key found. Please set it as an environment variable or in main.py\")\n",
    "\n",
    "# Function to generate queries using OpenAI's ChatGPT\n",
    "def generate_queries_chatgpt(original_query):\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates multiple search queries based on a single input query.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate multiple search queries related to: {original_query}\"},\n",
    "            {\"role\": \"user\", \"content\": \"OUTPUT (4 queries):\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    generated_queries = response.choices[0][\"message\"][\"content\"].strip().split(\"\\n\")\n",
    "    return generated_queries\n",
    "\n",
    "# Mock function to simulate vector search, returning random scores\n",
    "def vector_search(query, all_documents):\n",
    "    available_docs = list(all_documents.keys())\n",
    "    random.shuffle(available_docs)\n",
    "    selected_docs = available_docs[:random.randint(2, 5)]\n",
    "    scores = {doc: round(random.uniform(0.7, 0.9), 2) for doc in selected_docs}\n",
    "    return {doc: score for doc, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "\n",
    "# Reciprocal Rank Fusion algorithm\n",
    "def reciprocal_rank_fusion(search_results_dict, k=60):\n",
    "    fused_scores = {}\n",
    "    print(\"Initial individual search result ranks:\")\n",
    "    for query, doc_scores in search_results_dict.items():\n",
    "        print(f\"For query '{query}': {doc_scores}\")\n",
    "        \n",
    "    for query, doc_scores in search_results_dict.items():\n",
    "        for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n",
    "            if doc not in fused_scores:\n",
    "                fused_scores[doc] = 0\n",
    "            previous_score = fused_scores[doc]\n",
    "            fused_scores[doc] += 1 / (rank + k)\n",
    "            print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank} in query '{query}'\")\n",
    "\n",
    "    reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n",
    "    print(\"Final reranked results:\", reranked_results)\n",
    "    return reranked_results\n",
    "\n",
    "# Dummy function to simulate generative output\n",
    "def generate_output(reranked_results, queries):\n",
    "    return f\"Final output based on {queries} and reranked documents: {list(reranked_results.keys())}\"\n",
    "\n",
    "\n",
    "# Predefined set of documents (usually these would be from your search database)\n",
    "all_documents = {\n",
    "    \"doc1\": \"Climate change and economic impact.\",\n",
    "    \"doc2\": \"Public health concerns due to climate change.\",\n",
    "    \"doc3\": \"Climate change: A social perspective.\",\n",
    "    \"doc4\": \"Technological solutions to climate change.\",\n",
    "    \"doc5\": \"Policy changes needed to combat climate change.\",\n",
    "    \"doc6\": \"Climate change and its impact on biodiversity.\",\n",
    "    \"doc7\": \"Climate change: The science and models.\",\n",
    "    \"doc8\": \"Global warming: A subset of climate change.\",\n",
    "    \"doc9\": \"How climate change affects daily weather.\",\n",
    "    \"doc10\": \"The history of climate change activism.\"\n",
    "}\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    original_query = \"impact of climate change\"\n",
    "    generated_queries = generate_queries_chatgpt(original_query)\n",
    "    \n",
    "    all_results = {}\n",
    "    for query in generated_queries:\n",
    "        search_results = vector_search(query, all_documents)\n",
    "        all_results[query] = search_results\n",
    "    \n",
    "    reranked_results = reciprocal_rank_fusion(all_results)\n",
    "    \n",
    "    final_output = generate_output(reranked_results, generated_queries)\n",
    "    \n",
    "    print(final_output)"
   ],
   "id": "50550be37cbeebe3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
